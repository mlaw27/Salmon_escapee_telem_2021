library(ggpubr)
library(mapproj)
library(viridis)
library(actel)
library(gWidgets2)
library(RGtk2)
library(gWidgets2)
Telem_master_file_adjust <-read_csv('C:/Users/LAWRENCEM/OneDrive - DFO-MPO/Documents/Telelmetry stuff/2021 escapee_data/Outputs/master_files/Telemetry_master_adjusted_2021_June 6 2022.csv') %>% print()
depth_df <-Telem_master_file_adjust_depth %>%
select(date_time,transmitter,sensor_value, receiver, release_date_time) %>% print()
Telem_master_file_adjust_depth <- Telem_master_file_adjust %>% filter(sensor == 'depth') %>% print()
depth_df <-Telem_master_file_adjust_depth %>%
select(date_time,transmitter,sensor_value, receiver, release_date_time) %>% print()
depth_df.1 <- depth_df %>%
mutate(time_diff = as.numeric(difftime( date_time, release_date_time), units="hours")) %>%
print()
depth_df.2<-depth_df.1 %>% mutate(Bin_time = cut(time_diff,
breaks=c(0,1,2,4, 6, 8, 12, 18, 24,
36,48,72,96))) %>%
print()
depth_df.3<-depth_df.2 %>% group_by(transmitter, Bin_time) %>%
dplyr::summarize(Mean = mean(sensor_value , na.rm=TRUE)) %>% print()
depth_df.4_mean <-depth_df.3 %>% group_by(Bin_time) %>%
dplyr::summarize(Mean_collective = mean(Mean, na.rm=TRUE),
SD = sd(Mean, na.rm=TRUE),
Count =n()
) %>% drop_na(Bin_time) %>%
print()
#make labels for graph
Bin_time_labels <- as_tibble(c('0-1', '1-2', '2-4', '4-6', '6-8', '8-12', '12-18',
'18-24', '24-36', '36-48','48-72', '72-96')) %>%
rename(Time_range =value) %>% print()
depth_df.4_mean.1$Time_range<-as.factor(depth_df.4_mean.1$Time_range)
depth_df.4_mean.1$Time_range <- factor(depth_df.4_mean.1$Time_range, levels=c('0-1', '1-2', '2-4', '4-6', '6-8', '8-12', '12-18',
'18-24', '24-36', '36-48','48-72', '72-96'))
windowsFonts(Times=windowsFont("TT Times New Roman"))
#bind the two together
depth_df.4_mean.1 <-bind_cols(depth_df.4_mean, Bin_time_labels) %>% print()
depth_df.4_mean.1$Time_range <- factor(depth_df.4_mean.1$Time_range, levels=c('0-1', '1-2', '2-4', '4-6', '6-8', '8-12', '12-18',
'18-24', '24-36', '36-48','48-72', '72-96'))
Depth_histo<-depth_df.1 %>% mutate(Bin_time = cut(time_diff,
breaks=c(0, 12, 24,
48,96))) %>%
print()
Depth_histo.1<-Depth_histo %>% filter( !is.na(Bin_time)) %>%
mutate('Time (h)' = case_when(Bin_time == '(0,12]'~ '0-12',
Bin_time == '(12,24]' ~ '12-24',
Bin_time == '(24,48]' ~'24-48',
Bin_time == '(48,96]'  ~'48-96',
T ~ 'N')) %>%
print()
bins <- 50
cols <- c("lightblue1" ,"blue4")
colGradient <- colorRampPalette(cols)
cut.cols <- colGradient(bins)
cuts <- cut(Depth_histo_12$sensor_value,bins)
Depth_histo.1$`Time (h)`<-as.factor(Depth_histo.1$`Time (h)`)
Depth_histo.1
bins <- 50
cols <- c("lightblue1" ,"blue4")
colGradient <- colorRampPalette(cols)
cut.cols <- colGradient(bins)
cuts <- cut(Depth_histo_12$sensor_value,bins)
ggplot(Depth_histo.1, aes(x=sensor_value, fill = cut(sensor_value, bins))) +
geom_histogram( binwidth = 2.5,boundary = 0, bins = 50)+
labs(x=expression('Depth (m)'),
y=expression('Count'))+
theme_bw()+
coord_flip()+
theme(panel.border = element_blank(), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"))+
theme(axis.text.x = element_text(color = "black", size = 18,  face = "plain"),
axis.text.y = element_text(color = "grey20", size = 18, face = "plain"),
axis.title.x = element_text(color = "black", size = 24, hjust = .5, vjust = 0, face = 'bold'),
axis.title.y = element_text(color = "black", size = 26,  vjust = 2,  face = "bold"),
text=element_text(family="Times"),
legend.position="none",
plot.margin = margin(1,1,1.5,1.2, "cm"))+
scale_y_continuous( position = 'right')+
scale_x_reverse( expand = c(0,0),breaks = seq(0,80, by = 5), limits = c( 80, 0))+
scale_color_manual(values=cut.cols) +
scale_fill_manual(values=cut.cols)+
facet_grid(~`Time (h)`)+
theme(
strip.text = element_text(face = "bold", size = rel(1.5)),
strip.background = element_rect(fill = "white", colour = "grey", size = 1)
)
Telem_master_file_adjust <-read_csv('C:/Users/LAWRENCEM/OneDrive - DFO-MPO/Documents/Telelmetry stuff/2021 escapee_data/Outputs/master_files/Telemetry_master_adjusted_2021_June 6 2022.csv') %>% print()
Telem_master_file_adjust_depth <- Telem_master_file_adjust %>% filter(sensor == 'depth') %>% print()
library(ggplot2)
library(dplyr)
library(tidyverse)
library(lubridate)
library(stringr)
library(janitor)
library(ggrepel)
library(rgdal)
library(scales)
library(glatos)
library(ggmap)
library(ggpubr)
library(mapproj)
library(viridis)
library(actel)
library(gWidgets2)
library(RGtk2)
library(gWidgets2)
Telem_master_file_adjust <-read_csv('C:/Users/LAWRENCEM/OneDrive - DFO-MPO/Documents/Telelmetry stuff/2021 escapee_data/Outputs/master_files/Telemetry_master_adjusted_2021_June 6 2022.csv') %>% print()
Telem_master_file_adjust_depth <- Telem_master_file_adjust %>% filter(sensor == 'depth') %>% print()
depth_df <-Telem_master_file_adjust_depth %>%
select(date_time,transmitter,sensor_value, receiver, release_date_time) %>% print()
depth_df.1 <- depth_df %>%
mutate(time_diff = as.numeric(difftime( date_time, release_date_time), units="hours")) %>%
print()
depth_df.2<-depth_df.1 %>% mutate(Bin_time = cut(time_diff,
breaks=c(0,1,2,4, 6, 8, 12, 18, 24,
36,48,72,96))) %>%
print()
view(depth_df.2)
depth_df.3<-depth_df.2 %>% group_by(transmitter, Bin_time) %>%
dplyr::summarize(Mean = mean(sensor_value , na.rm=TRUE)) %>% print()
depth_df.4_mean <-depth_df.3 %>% group_by(Bin_time) %>%
dplyr::summarize(Mean_collective = mean(Mean, na.rm=TRUE),
SD = sd(Mean, na.rm=TRUE),
Count =n()
) %>% drop_na(Bin_time) %>%
print()
#bind the two together
depth_df.4_mean.1 <-bind_cols(depth_df.4_mean, Bin_time_labels) %>% print()
depth_df.4_mean.1$Time_range<-as.factor(depth_df.4_mean.1$Time_range)
depth_df.4_mean.1$Time_range <- factor(depth_df.4_mean.1$Time_range, levels=c('0-1', '1-2', '2-4', '4-6', '6-8', '8-12', '12-18',
'18-24', '24-36', '36-48','48-72', '72-96'))
#bind the two together
depth_df.4_mean.1 <-bind_cols(depth_df.4_mean, Bin_time_labels) %>% print()
depth_df.range.2<-depth_df.range %>% group_by(transmitter, Bin_time) %>%
dplyr::summarize(Max_val = max(sensor_value , na.rm=TRUE),
Min_val = min(sensor_value, na.rm= T)) %>% print()
depth_df.range<-depth_df.1 %>% mutate(Bin_time = cut(time_diff,
breaks=c(0,1,2,4, 6, 8, 12, 18, 24,
36,48,72,96))) %>%
print()
depth_df.range.2<-depth_df.range %>% group_by(transmitter, Bin_time) %>%
dplyr::summarize(Max_val = max(sensor_value , na.rm=TRUE),
Min_val = min(sensor_value, na.rm= T)) %>% print()
Range_mean <-depth_df.range.2 %>% group_by(Bin_time) %>%
dplyr::summarize(Mean_max = mean(Max_val , na.rm=TRUE),
SD_max = sd(Max_val, na.rm=TRUE),
Mean_min = mean(Min_val , na.rm=TRUE),
SD_min = sd(Min_val, na.rm=TRUE),
Count =n()
) %>% drop_na(Bin_time) %>%
print()
Range_mean
depth_df.range.3 <-depth_df.range.2 %>% left_join(Range_mean,.) %>%
print()
library(ggplot2)
library(dplyr)
library(tidyverse)
library(lubridate)
library(stringr)
library(janitor)
library(ggrepel)
library(rgdal)
library(scales)
library(glatos)
library(ggmap)
library(ggpubr)
library(mapproj)
library(viridis)
library(actel)
library(gWidgets2)
library(RGtk2)
library(gWidgets2)
library(geosphere)
Master_res <-read_csv('C:/Users/LAWRENCEM/OneDrive - DFO-MPO/Documents/Telelmetry stuff/2021 escapee_data/Actel analyses/Dec 2022_work with this/data_outputs/Residency stuff/Residency complete_Dec 13 2022.csv') %>% print()
crude_lats <-read_csv('C:/Users/LAWRENCEM/OneDrive - DFO-MPO/Documents/Telelmetry stuff/2021 escapee_data/Actel analyses/Dec 2022_work with this/Master_files_Dec 2022/Prepped_raw_data_files_Dec 8 2022/Distance_Lat_long.csv') %>% print()
#change name to make joinable
crude_lats.1 <-crude_lats %>%
rename(First.array = Array) %>% print()
#Put in a col with a representaive start point
crude_lats.2<-crude_lats.1 %>%mutate(Start_long = -67.0133, Start_lat = 44.9645) %>%
print()
#calcuate distance between start point and array point
#default output is metres, adding in one for km
crude_lats.3<-crude_lats.2 %>% mutate(  Distance_m=distHaversine( cbind(Long_rep, Lat_rep ), cbind(Start_long, Start_lat )  )) %>%
mutate(Distance_km = Distance_m/1000) %>%
print()
length(unique(crude_lats.3$Distance_km))
length(unique(crude_lats.3$First.array))
#ok above looks fine. Also did a couple manual distances to make sure distances
#are being computed properly. That was done in G Earth
#merge distances with the residency data:
Master_res_max_dist <-Master_res %>% left_join(crude_lats.3) %>% print()
#give max distances
Master_res_max_dist.1<-Master_res_max_dist %>% group_by(Transmitter) %>%
slice(which.max(Distance_km)) %>%
print()
#stats output for overall trends
Master_res_max_dist.1 %>% ungroup() %>%
summarize(median(Distance_km),
mean(Distance_km))
library(car)
library(tidyr)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(metafor)
library(gridExtra)
library(forestplot)
library(ggforestplot)
DF_A<-read_csv("R:/Science/CESD/BES_MarcTrudel/Tag retention study_2021/meta analysis/Primary data files_dec 2021/Meta_data_filtering_Jan 11 2022_currated.csv") %>% print()
DF_A.1 <- DF_A %>% filter(Reject == "N")  %>% print()
DF_A.2<-DF_A.1 %>% select('Paper ID...19','Year_2', 'Authors...21', 'Species', 'Common_name',
'Water type', 'Treatment', 'Tag type', 'Sample Size (n)',
'Num Tag lost', 'Tag_reten_perc', 'Total Mortalities', 'Mort_perc',
'Exp_Duration', 'Tag_length', 'Tag_diam', 'other_tags',
'Incision_size_max_mm', "tag_mass_air", "Fish_mass_ave", 'Notes') %>%
rename(Paper_ID = 'Paper ID...19', Year = Year_2, Author = 'Authors...21', Water_type= 'Water type',
Tag_type = 'Tag type', Sample_size = 'Sample Size (n)',Tag_loss_N = 'Num Tag lost',
Mort_N = 'Total Mortalities', Incision_size = 'Incision_size_max_mm') %>%
print()
DF_A.2 %>% summarise(Unique_Elements = n_distinct(Paper_ID))
Meta_data <- read_csv('R:/Science/CESD/BES_MarcTrudel/Tag retention study_2021/meta analysis/Stats output/Tag_meta filtered_Jan 26 2022.csv') %>% print()
write_csv(DF_A.2, 'R:/Science/CESD/BES_MarcTrudel/Tag retention study_2021/meta analysis/Stats output/Tag_meta filtered_Jan 26 2022.csv')
Meta_data <- read_csv('R:/Science/CESD/BES_MarcTrudel/Tag retention study_2021/meta analysis/Stats output/Tag_meta filtered_Jan 26 2022.csv') %>% print()
#dropping any controls or sham fish from the df
Tagged_fish_DF <- Meta_data.1 %>% filter(!grepl('control|sham', Tag_type)) %>% print()
Meta_data.1 <- Meta_data %>% print()
#dropping any controls or sham fish from the df
Tagged_fish_DF <- Meta_data.1 %>% filter(!grepl('control|sham', Tag_type)) %>% print()
Tagged_fish_DF.1 <-Tagged_fish_DF %>% select(-Notes, -Treatment, -Tag_diam, -Species, -Incision_size, -Water_type, -other_tags, -Common_name) %>%
mutate(Tags_retained = Sample_size-Tag_loss_N) %>% mutate(Tag_BW_ratio = tag_mass_air/Fish_mass_ave ) %>% print()
CD<-Tagged_fish_DF.1 %>% drop_na(Tags_retained)
CD<-Tagged_fish_DF.1 %>% drop_na(Tags_retained) %>% print()
#overall mortility from all of the tagged fish
mean(Tagged_fish_DF.1$Mort_perc, na.rm =T)
range(Tagged_fish_DF.1$Mort_perc, na.rm =T)
Tagged_fish_DF.1
view(Tagged_fish_DF.1)
DF_A.2
view(DF_A.2)
library(car)
library(tidyr)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(metafor)
library(gridExtra)
library(forestplot)
library(ggforestplot)
library(readxl)
library(janitor)
DF_A<-read_excel("R:/Science/CESD/BES_MarcTrudel/Tag retention study_2021/meta analysis/Primary data files_dec 2021/Meta_data_filtering_Jan 20 2023_currated.xlsx") %>% print()
#318 x45
names(DF_A)
DF_A.1 <- DF_A %>% filter(Reject == "N")  %>% print()
#162 x 44
names(DF_A.1)
DF_A.2<-DF_A.1 %>% clean_names() %>% print()
names(DF_A.2)
#take out what I need and rename so stuff
DF_A.3<-DF_A.2 %>%
select( "paper_id_19","year_2", "authors_21",'taxa_18',"species","common_name_2",
"water_type","treatment", "tag_type_2", "sample_size_n",
"num_tag_lost", "tag_reten_perc","total_mortalities","mort_perc",
"exp_duration","tag_length","tag_diam","other_tags_2",
"incision_size_max_mm",
"tag_mass_air","fish_mass_ave","fish_mass_ave_actual", 'notes') %>%
rename( paper_id = 'paper_id_19', year = year_2, author = 'authors_21',
tag_type = 'tag_type_2',tag_loss_N = 'num_tag_lost',
mort_N = 'total_mortalities', incision_size = 'incision_size_max_mm',
taxa = 'taxa_18') %>%
print()
#162 x 23
DF_A.3 %>% summarise(Unique_Elements = n_distinct(paper_id))
#40 total papers that are included in the analysis
#writing out the data file with the filtered results
write_csv(DF_A.3, 'C:/Users/LAWRENCEM/OneDrive - DFO-MPO/Documents/Telelmetry stuff/tag retention/meta analysis/2023_stuff/Tag_meta filtered_Jan 25 2023.csv')
Meta_data <- read_csv('C:/Users/LAWRENCEM/OneDrive - DFO-MPO/Documents/Telelmetry stuff/tag retention/meta analysis/2023_stuff/Tag_meta filtered_Jan 25 2023.csv') %>% print()
#162x23
####Calc'ing the stand error for tag loss
#dropping any controls or sham fish from the df
Tagged_fish_DF <- Meta_data %>% filter(!grepl('control|sham', tag_type)) %>% print()
#111x23
sort(Tagged_fish_DF$tag_type) %>% print() #visual check shows no control fish
str(Tagged_fish_DF)
length(Tagged_fish_DF$tag_loss_N) #110
C<-Tagged_fish_DF %>% drop_na(tag_loss_N)
C #80
#simplifying and reorganizing DF
Tagged_fish_DF.1 <-Tagged_fish_DF %>% select(-notes, -other_tags_2) %>%
separate(species, c("genus", "species"), " ") %>%
mutate(tags_retained = sample_size_n-tag_loss_N) %>% mutate(tag_BW_ratio = tag_mass_air/fish_mass_ave ) %>% print()
#110x23
CD<-Tagged_fish_DF.1 %>% drop_na(tags_retained) %>% print()
CD #80x15
#####Retention estimates######
names(Tag_retention_DF)
mean(Tag_retention_DF$tag_reten_perc, na.rm = T)
#[1] 86.69237
range(Tag_retention_DF$tag_reten_perc, na.rm = T)
#[1]  20 100
Tag_retention_DF<-  escalc(measure = "PFT", ni=sample_size_n,
xi=tags_retained,
data =Tagged_fish_DF.1, add = 0)
Tag_retention_DF
dim(Tag_retention_DF) #110x17
dim(Tag_retention_DF %>% drop_na(yi))
#80, looks like it all checks out
#Yi is transformed proportion, vi is the sampling variance
####Base retention model (no covariates)######
Base_Reten_model <- rma.mv(yi, vi,random = ~1|paper_id, method = "REML", data = Tag_retention_DF)
Base_Reten_model
# Multivariate Meta-Analysis Model (k = 80; method: REML)
#
# Variance Components:
#
#   estim    sqrt  nlvls  fixed    factor
# sigma^2    0.0396  0.1990     35     no  paper_id
#
# Test for Heterogeneity:
#   Q(df = 79) = 1235.1300, p-val < .0001
#
# Model Results:
#
#   estimate      se     zval    pval   ci.lb   ci.ub
# 1.2745  0.0361  35.3538  <.0001  1.2039  1.3452  ***
#
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#
#
profile(Base_Reten_model)
#looks alright!
#from this: http://www.metafor-project.org/doku.php/analyses:konstantopoulos2011
# Both profile likelihood plots are peaked at the respective parameter estimates
# (as indicated by the vertical dotted lines) and
# the log likelihoods quickly decrease (i.e., become more negative)
# as the values of the components are moved away from the actual REML estimates.
# Hence, we can be fairly confident that both variance components are identifiable.
#IMPORTANT:
# #When profiling a variance component (or some other parameter that cannot be negative),
# the peak may be at zero (if this corresponds to the ML/REML estimate of the parameter).
# In this case, the profiled log-likelihood should be a monotonically
# decreasing function of the parameter.
#https://wviechtb.github.io/metafor/reference/profile.rma.html
#
# In essence, the meta-analytic random-effects model can be conceptualized
# as a multilevel model with the true effects at level 2 and the observed effects
# at level 1. Using typical multilevel model terminology, the random = ~ 1 | trial
# argument adds random intercepts at level 2 to the model. Note that the variance
# of the true effects (commonly denoted as σ2in in the meta-analytic literature)
#is denoted as the output above (i.e., the value of
# σ2 given in the output is the estimated variance of the random intercepts).
#
τ
2
#From the Viectbacher paper:
#Residual heterogeneity is signifcant (QE = 28:33, df = 10, p < 0:01),
#possibly indicating that
# other moderators not considered in the model are
#infuencing the vaccine effectiveness
#The results given under "Test of Moderators" provides a joint
#or omnibus test of all model coefficients except for the intercept
#quick regression to make sure tag length isn't correlated with the tag/BR ratio
GLM_BWrat<-glm(data = Tag_retention_DF, tag_BW_ratio~tag_length)
summary(GLM_BWrat)
# Call:
#   glm(formula = tag_BW_ratio ~ tag_length, data = Tag_retention_DF)
#
# Deviance Residuals:
#   Min         1Q     Median         3Q        Max
# -0.018911  -0.011350  -0.004420   0.004147   0.073890
#
# Coefficients:
#   Estimate Std. Error t value Pr(>|t|)
# (Intercept)  1.922e-02  3.775e-03   5.091 3.19e-06 ***
#   tag_length  -4.602e-06  1.184e-04  -0.039    0.969
# ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#
# (Dispersion parameter for gaussian family taken to be 0.0002685143)
#
# Null deviance: 0.017722  on 67  degrees of freedom
# Residual deviance: 0.017722  on 66  degrees of freedom
# (42 observations deleted due to missingness)
# AIC: -362.19
#
# Number of Fisher Scoring iterations: 2
#
# >
###ALL GOOD, no sig relationship between the two parameters
######Model looking at covaraites in retention
###Full model#####
names(Tag_retention_DF)
Reten_model_full <- rma.mv(yi, vi,mods = ~tag_length+tag_diam+incision_size+exp_duration+tag_BW_ratio,
random = ~1|paper_id, method = "REML", data = Tag_retention_DF)
Reten_model_full
Reten_model_full <- rma.uni(yi, vi,mods = ~tag_length+tag_diam+incision_size+exp_duration+tag_BW_ratio,
random = ~1|paper_id, method = "REML", data = Tag_retention_DF)
Reten_model_full
Reten_model_full <- rma.mv(yi, vi,mods = ~tag_length+tag_diam+incision_size+exp_duration+tag_BW_ratio,
random = ~1|paper_id, method = "REML", data = Tag_retention_DF)
Reten_model_full
Reten_model_full <- rma(yi, vi,mods = ~tag_length+tag_diam+incision_size+exp_duration+tag_BW_ratio,
random = ~1|paper_id, method = "REML", data = Tag_retention_DF)
Reten_model_full
Reten_model_full <- rma.mv(yi, vi,mods = ~tag_length+tag_diam+incision_size+exp_duration+tag_BW_ratio,
random = ~1|paper_id, method = "REML", data = Tag_retention_DF)
Reten_model_full
Reten_model_full$tau2
Reten_model_full <- rma.mv(yi, vi,mods = ~tag_length+tag_diam+incision_size+exp_duration+tag_BW_ratio,
random = ~1|paper_id, method = "REML", data = Tag_retention_DF)
W <- diag(1/Reten_model_full$vi)
X <- model.matrix(Reten_model_full)
X <- model.matrix(Reten_model_full) %>% print()
W <- diag(1/Reten_model_full$vi) %>% print()
X <- model.matrix(Reten_model_full) %>% print()
P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
100 * sum(Reten_model_full$sigma2) / (sum(Reten_model_full$sigma2) + (Reten_model_full$k-Reten_model_full$p)/sum(diag(P)))
100 * Reten_model_full$sigma2 / (sum(Reten_model_full$sigma2) + (Reten_model_full$k-Reten_model_full$p)/sum(diag(P)))
ggplot(Tag_retention_DF, aes(x=tag_BW_ratio, y=yi))+geom_point()
ggplot(Tag_retention_DF, aes(x=tag_BW_ratio, y=yi))+geom_point()+geom_smooth(method='lm', formula= y~x)
Reten_model_Length <- rma.mv(yi, vi,mods = ~tag_length,
random = ~1|paper_id, method = "REML", data = Tag_retention_DF)
Reten_model_Length
Reten_model_Diam <- rma.mv(yi, vi,mods = ~tag_diam,
random = ~1|paper_id, method = "REML", data = Tag_retention_DF)
Reten_model_Length
Reten_model_Diam
Tag_retention_DF
names(Tag_retention_DF)
Reten_model_Dur <- rma.mv(yi, vi,mods = ~exp_duration,
random = ~1|paper_id, method = "REML", data = Tag_retention_DF)
Reten_model_Dur
Reten_model_BWratio <- rma.mv(yi, vi,mods = ~tag_BW_ratio,
random = ~1|paper_id, method = "REML", data = Tag_retention_DF)
Reten_model_BWratio
Reten_model_incision <- rma.mv(yi, vi,mods = ~incision_size,
random = ~1|paper_id, method = "REML", data = Tag_retention_DF)
Reten_model_incision
library(ggplot2)
library(dplyr)
library(tidyverse)
library(lubridate)
library(stringr)
library(janitor)
library(ggrepel)
library(rgdal)
library(scales)
library(glatos)
library(ggmap)
library(ggpubr)
library(mapproj)
library(viridis)
library(actel)
library(gWidgets2)
library(RGtk2)
library(gWidgets2)
#read in datafile
Master_res_all <- read_csv('C:/Users/LAWRENCEM/OneDrive - DFO-MPO/Documents/Telelmetry stuff/2021 escapee_data/Actel analyses/Dec 2022_work with this/data_outputs/Residency stuff/Residency complete_Dec 13 2022.csv') %>%
print()
setwd('C:/Users/LAWRENCEM/OneDrive - DFO-MPO/Documents/Telelmetry stuff/2021 escapee_data/Actel analyses/Dec 2022_work with this/Master_files_Dec 2022')
Res_df<-residency(tz = "UTC", jump.warning = 2,
timestep = "hours",
save.detections = T,
save.tables.locally = TRUE, report = T,  max.interval = 15,
section.error = 0,
section.warning = 0,
inactive.error = 10,
override = 'A69-9007-6752')
Res_df<-residency(tz = "UTC", jump.warning = 2,
timestep = "hours",
save.detections = T,
save.tables.locally = TRUE, report = T,  max.interval = 15,
section.error = 0,
section.warning = 0,
inactive.error = 10,
override ='6752')
library(actel)
Res_df<-residency(tz = "UTC", jump.warning = 2,
timestep = "hours",
save.detections = T,
save.tables.locally = TRUE, report = T,  max.interval = 15,
section.error = 0,
section.warning = 0,
inactive.error = 10,
override = 6752)
dataToList(actel_residency_results)
C<-dataToList('actel_residency_results.RData') %>% print()
C<-dataToList('actel_residency_results.RData') %>% names()
names(C)
C<-dataToList('actel_residency_results.RData')
names(C)
V<-residency(tz = 'UTC', section.order = NULL, datapack = NULL, max.interval = 15, min.total.detections = 2, min.per.event = c('1'), start.time = NULL, stop.time = NULL, speed.method = c('last to first'), speed.warning = NULL, speed.error = NULL, jump.warning = 2, jump.error = 3, inactive.warning = 10, exclude.tags = NULL, override = c(6752), report = TRUE, auto.open = TRUE, discard.orphans = FALSE, discard.first = NULL, save.detections = TRUE, section.warning = 0, section.error = 0, timestep = 'hours', replicates = NULL, inactive.error = 10, GUI = 'never', save.tables.locally = TRUE, print.releases = TRUE, detections.y.axis = 'auto')
y # reuse detections?
comment # on A69-9007-6752
kkkkkkkkk
y # invalidate/expand moves in A69-9007-6752?
expand
2 # Expand this event
n # invalidate detections in event 2 of A69-9007-6752?
y # invalidate/expand moves in A69-9007-6752?
13:19
y # confirm?
comment # on A69-9007-6752
predation event
n # invalidate more?
n # s
V<-residency(tz = 'UTC', section.order = NULL, datapack = NULL, max.interval = 15, min.total.detections = 2, min.per.event = 1, start.time = NULL, stop.time = NULL, speed.method = c('last to first'), speed.warning = NULL, speed.error = NULL, jump.warning = 2, jump.error = 3, inactive.warning = 10, exclude.tags = NULL, override = c(6752), report = TRUE, auto.open = TRUE, discard.orphans = FALSE, discard.first = NULL, save.detections = TRUE, section.warning = 0, section.error = 0, timestep = 'hours', replicates = NULL, inactive.error = 10, GUI = 'never', save.tables.locally = TRUE, print.releases = TRUE, detections.y.axis = 'auto')
head(C$status.df)
head(Res_df$status.df)
